---
title: "FishBae"
subtitle: "EEMB 595 Final Project" 
author: "Phoebe Racine & Erin Winslow"
date: "5/13/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Task List for Phoebe & Erin
- ~~copy/paste from Juanca~~
  - ~~figure out what data sources we need in order to create Bayesian regression~~
  - ~~I think we're at "fao_salmon_lobster.csv" & "all_salmon_lobster.csv"~~
- ~~copy/paste GLM code from R~~
- **research GLMs**
  - We need to decide what kind of GLM we should use
  - alpha
  - beta
  - C
  - ~~priors~~
    - there are some that are a lot of people use
    - normal prior might be a good place to start
  - distribution
  - read 3.3 of stats book
  - figure out if we need to use random error graph
- **Code GLM in STAN**
  - I think this will give us something to present to the class that's might be new to them and is in reach because Juan Carlos and Dan Ovando know how to use it. Grace is also willing to help.
  - download RSta interface
- **update GLM code to fit our data needs**
  - ~~select by salmon and lobster~~
  - ~~make data frame for GLM~~
  - why is regression line at 0?
  - go through tidying of data in order to better mirror Juanca graph
- **combine Bay line w/ Juanca graph**
- **For Grace's analysis**
  - create RMarkdown document ready to turn in
  - beautify graph
  - beautify regression table
- **Presentation**
  - pull graphs from early AquaForce


#Considering Approach

###What kind of GLM?
**Poisson Generalized Linear Model**: a generalized linear model form of regression analysis used to model count data and contingency tables. Poisson regression assumes the response variable Y has a Poisson distribution, and assumes the logarithm of its expected value can be modeled by a linear combination of unknown parameters. A Poisson regression model is sometimes known as a log-linear model, especially when used to model contingency tables.
- We're treating quantities of aquaculture production and fisheries catch as count data.

**Poisson distribution**: a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and independently of the time since the last event (Frank A. Haight (1967). Handbook of the Poisson Distribution. New York: John Wiley & Sons).

--**since production is a constant, we may want to switch to a different distribution**
  - log OR normal could be a better fit

*Log-normal distribution*: a continuous probability distribution of a random variable whose logarithm is normally distributed. Thus, if the random variable X is log-normally distributed, then Y = ln(X) has a normal distribution. Likewise, if Y has a normal distribution, then the exponential function of Y, X = exp(Y), has a log-normal distribution. A random variable which is log-normally distributed takes only positive real values.

#Loading Packages
```{r, include=FALSE}
library(tidyverse)
library(stargazer)
library(ggplot2)
# install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies=TRUE)

```


#Salmon and Lobster Code from Juanca
- Salmon-lobsert graph
- code for GLM in R
- code for GLM in JAGS

##Salmon-Lobster Graph
```{r}

fao_salmon_lobster  %>%
  group_by(year, source, general_name) %>% 
  summarize(quantity = sum(quantity)) %>% 
  filter(!(general_name == "Lobster" & source == "Aquaculture")) %>% 
  rename(Species = general_name) %>% 
  ggplot(aes(x = year, y = quantity/1000000, color = source, linetype = Species, group = paste(source, Species))) +
  geom_line(size = 1) +
  scale_color_brewer(palette = "Set1") +
  geom_vline(xintercept = 1962, linetype = "dashed") +
  geom_text(aes(x = 1975, y = 2.5, label = "Salmon aquaculture begins"), color = "black", size = 4) +
  labs(x = "Year", y = "Quantities (million MT)", caption = "(Data from FAO: www.fao.org/fishery/statistics/)") +
  theme_minimal()

```


#It's nice to know the difference between R and JAGS but Grace confirmed she wants us to do in JAGS, STAN or a specifc bayesian package -- we can explain why in our presentation
#Code for GLM in R
**Poisson GLM in R and WinBUGS for modeling time series of counts**
The poisson GLM is described by a noise or random parameter following a Poisson distribution. In this sense, the random part of the response (or the statistical distribution) is given by:

Ci∼Poisson(λi)
Here, λi is the expected count (mean response) related to the response. It is the link between the systematic and stochastic parts of the GLM:
C = What is C?
i = number of years

log(λi)=ηiλi=eηi

And ηi is simply the linear predictor (the systematic or signal part of the response):

ηi=α+β∗Xi

Intuitively, α and β are the parameters of interest, and Xi represents the value of the covariate (predictor variable) X at year i.

###Visualizing the Random Error introduced through a Poisson Process
- *need to determine if this is important or nice to have*
```{r}

###everything needs to be switched out to fit our work
data_fn() %>% #need to change
  ggplot(aes(x = year, y = C)) +
  geom_line(aes(y = expected_count), size = 1) + 
  geom_point(shape = 21, fill = "steelblue", size = 4) +
  labs(x = "year", y = "Population size")


```
Black line represents the signal, blue dots represent the signal plus the random error introduced through a poisson process.



#Our Model
Count = "quantity"
Years = 65 years, 1950 --> 2015
Total data points: 13,293




###R Approach
In R, we apply a GLM using the glm function, to which we must specify the formula (the systematic part) and the name of the family that generates the error distribution.


```{r}


###everything needs to be switched out to fit our work
set.seed(43) #The set.seed you use is the starting number you use in a sequence of numbers from random generation. How should we pick the set.seed?

data <- data_fn()

fm <- glm(C ~ year + I(year^2) + I(year^3), family = "poisson", data = data)

stargazer::stargazer(fm,
                     type = "html",
                     header = F,
                     single.row = T,
                     intercept.bottom = F,
                     intercept.top = T)

data <- data_fn()

fm <- glm(C ~ year + I(year^2) + I(year^3), family = "poisson", data = data)

stargazer::stargazer(fm,
                     type = "html",
                     header = F,
                     single.row = T,
                     intercept.bottom = F,
                     intercept.top = T)


```

####Our test
```{r}

salmon_lobster <- fao_salmon_lobster %>%
  mutate(binary = ifelse(general_name == "Salmon", 1, 0))
# View(salmon_lobster)  

modeltestBAY <- glm(binary ~ quantity * year + I(year^2) + I(year^3), family = "poisson", data = salmon_lobster)
summary(modeltestBAY)

modeltestFREQ <- glm(binary ~ quantity * year, family = "binomial", data = salmon_lobster)
summary(modeltestFREQ)

stargazer::stargazer(modeltestBAY,
                     type = "html",
                     header = F,
                     single.row = T,
                     intercept.bottom = F,
                     intercept.top = T)



```
##Saving predictions into dataframe
- we can also try: https://blogs.uoregon.edu/rclub/2016/04/05/plotting-your-logistic-regression-models/
```{r}

# save predictions of the model in the new data frame 
# together with variable you want to plot against
predictedBAY_df <- data.frame(samlob_pred = predict(modeltestBAY, salmon_lobster), hp=salmon_lobster$quantity)
#View(predictedBAY_df)

```


##Graphing our Model - Take 1
```{r}


ggplot(salmon_lobster, aes(year, quantity)) + 
      geom_point() +
    geom_line(color='red',data = fortify(predictedBAY_df), aes(x=samlob_pred, y=hp)) #what does fortify do?



```
##Graphing our Model - Take 2

```{r}

  ggplot(salmon_lobster, aes(year, quantity)) +       
  geom_point() +
  stat_smooth(method=glm, family=poisson, se = F)  #how do we tell it what model we're running?? #Family = binomial and family = poisson result in the same thing rn. #se = F means we're not including the standard errors



```

##STAN Approach
```{r}

data {
  int<lower=0> n;       // Number of years
  int<lower=0> C[n];    // Count
  vector[n] year;       // Year
}

transformed data {
  vector[n] year_squared;
  vector[n] year_cubed;

  year_squared = year .* year;
  year_cubed = year .* year .* year;
}

parameters {
  real<lower=-20,upper=20> alpha;
  real<lower=-10,upper=10> beta1;
  real<lower=-10,upper=10> beta2;
  real<lower=-10,upper=10> beta3;
}

transformed parameters {
  vector[n] log_lambda;

  log_lambda = alpha
             + beta1 * year +
             + beta2 * year_squared +
             + beta3 * year_cubed;
}

model {
  // Implicit uniform priors are used.

  // Likelihood
  C ~ poisson_log(log_lambda);
}

generated quantities {
  vector[n] lambda;

  lambda = exp(log_lambda);
}

```

```{r}

set.seed(43)
out <- stan(here::here("WiNBUGS", "stan_source", "GLM_Poisson.stan"),
            data = datab,
            init = inits,
            pars = params,
            chains = nc,
            thin = nt,
            iter = ni,
            warmup = nb,
            seed = 43,
            open_progress = FALSE)

```

We can inspect the values. If Rhat (R̂ ) values are all less than 1.1. This indicates convergence of the different MCMC chains was achieved. The R̂  values are obtained in an ANOVA-like way, where variance in parameter values is compared within and between chains.
```{r}
out
```


#####Plotting the estimates and their CRIs
```{r}

plot(out, pars = c("alpha", "beta1", "beta2", "beta3"), ci_level = 0.95, outer_level = 1)

```

Coefficient estimates and Credible intervals around them.

####Posterior distributions of our parameters
```{r}
mcmc <- rstan::extract(out)

pars <- mcmc[ c('alpha', 'beta1', 'beta2', 'beta3')] %>% 
  map_df(as_data_frame, .id = 'variable')

pars %>% 
  ggplot(aes(x = value, fill = variable)) + 
  geom_density(alpha = 0.5) + 
  facet_grid(~variable, scales = "free")

```






###JAGS Approach
First, we need to specify the model
```{r}
sink(here::here("WinBUGS", "GLM_Poisson.txt"))

cat("
  model{
    # Priors
    alpha ~ dunif(-20, 20)
    beta1 ~ dunif(-10, 10)
    beta2 ~ dunif(-10, 10)
    beta3 ~ dunif(-10, 10)
    
    #Likelihood: Note key components of a GLM on one line each
    
    for(i in 1:n){
      C[i] ~ dpois(lambda[i]) # This is the distribution for random part
      log(lambda[i]) <- log.lambda[i] # This is the link function
      log.lambda[i] <- alpha + beta1*year[i] + beta2*pow(year[i], 2) + beta3*pow(year[i], 3) # This is the linear predictor
    }
  }"
  , fill = T)

sink()


```


We now bundle the data. Since our predictor (year) is bounded between 0 and 40 and we have a cubic term, we would have a huge number on one side of the yea vector (i.e. 403=64000), and small numbers on the other. We must center and scale our data. The best way to do this is by taking the years, substracting the mean and then dividing by the standard deviation.
```{r}

y_trans <- (data$year - mean(data$year)) / sd(data$year)

datab <- list(C = data$C, n = length(data$C), year = y_trans)

```

And define a function that returns random initial values
```{r}
inits <- function(){
  list(alpha = runif(1, -2, 2),
       beta1 = runif(1, -3, 3))
}

```

We need to specify a list of the quantities we want to estimate or measure. These are the parameters that WinBUGS should return
```{r}

params <- c("alpha", "beta1", "beta2", "beta3", "lambda")

```

We must also specify a series of MCMC settings, including the number of draws per chain, thinning rate, burnin length, and number of chains.

The settings are:

```{r}

ni <- 2000
nt <- 2
nb <- 1000
nc <- 3

```

We can now pass all these to WinBUGS
```{r}
set.seed(43)
out_jags <- jags(data = datab,
                 inits = inits,
                 parameters.to.save = params,
                 model.file = "GLM_poisson.txt",
                 n.chains = nc,
                 n.thin = nt,
                 n.iter = ni,
                 n.burnin = nb,
                 parallel = T)


```

```{r}
out_jags

```


#Considering Using RAM Data

##Load RAM Legacy database
```{r}
ram <- read.csv(here("raw_data","RAM", "RAM.csv"), stringsAsFactors = F) %>% 
  janitor::clean_names()

```

```{r}
lobster_salmon_ram <- ram %>% 
  mutate(lobster = grepl(pattern = "lobster", x = stocklong),
         salmon = grepl(pattern = "salmon", x = stocklong),
         general_name = case_when(lobster ~ "Lobster",
                                  TRUE ~ "Salmon")) %>% 
  filter(lobster | salmon,
         tsid == "BdivBmsytouse-dimensionless")

```

###Graph of RAM Salmon and Lobster data
```{r}

lobster_salmon_ram %>% 
  group_by(tsyear, general_name) %>% 
  summarize(tsvalue = median(tsvalue, na.rm = T)) %>% 
  ggplot(aes(x = tsyear, y = tsvalue, color = general_name, group = general_name)) +
  geom_line(size = 1) +
  theme_bw()

```



